{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANFingerprint Deepfake Detection\n",
    "\n",
    "This notebook provides a complete walkthrough of training, evaluating, and using the GANFingerprint deepfake detection model. The model is designed to detect GAN-generated fake images by analyzing subtle fingerprint patterns in both spatial and frequency domains.\n",
    "\n",
    "## What are GAN Fingerprints?\n",
    "GAN Fingerprints are distinctive patterns or traces that are unintentionally embedded in images generated by Generative Adversarial Networks (GANs). These GAN fingerprints are akin to real human fingerprints, with the comparison that humans unintentionally leave fingerprints on the items they touch, that can be used to trace their identities. Just like human fingerprints, these GAN Fingerprints are unique to the GAN architecture the images are generated from, due to these factors:\n",
    "\n",
    "1. Each GAN architecture has its own unique way of generating images based on its specific design, loss functions, and optimization methods.\n",
    "\n",
    "2. Even GANs with identical architectures but different training datasets, random initializations, or hyperparameters will produce images with subtly different characteristics.\n",
    "\n",
    "## Objective of the project\n",
    "\n",
    "With GAN image generation images getting more advanced, there may be difficulties identifying deepfake images through existing methods, such as detecting distortions in facial features and image details. Through our project, we hope to create a deepfake detection model that can identify deepfake images reliably, no matter how realistic the generated images are to the human eye. By customizing and creating a model that can discriminate deepfake images from real ones through their GAN Fingerprint profiles, we hope to come up with a more sophisticated model which can capture details invisible to the human eye."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "#### First, let's install all necessary dependencies so that the model runs properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Virtual Environment for model\n",
    "!python3 -m venv myenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next: Activate the Virtual Environment (manually in terminal)\n",
    "\n",
    "Open a terminal in the notebook's folder, then run:\n",
    "\n",
    "- **On macOS/Linux/WSL**:\n",
    "  ```bash\n",
    "  source myenv/bin/activate\n",
    "  ```\n",
    "\n",
    "- **On Windows**:\n",
    "  ```cmd\n",
    "  myenv\\Scripts\\activate.bat\n",
    "  ```\n",
    "\n",
    "Once activated, your terminal prompt will show `(myenv)`, meaning you're using the virtual environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, install all necessary dependencies using requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation cell - Run this to install all necessary dependencies\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# Install PyTorch with CUDA support (needed for this model to run efficiently):\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's verify that we're running from the correct directory and all required files are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the current directory to the Python path\n",
    "sys.path.append('.')\n",
    "\n",
    "# Check if all required files exist\n",
    "files_needed = [\n",
    "    \"config.py\",\n",
    "    \"data_loader.py\",\n",
    "    \"models/__init__.py\",\n",
    "    \"models/fingerprint_net.py\", \n",
    "    \"models/layers.py\",\n",
    "    \"train.py\",\n",
    "    \"evaluate.py\",\n",
    "    \"inference.py\",\n",
    "    \"utils/metrics.py\",\n",
    "    \"utils/experiment.py\",\n",
    "    \"utils/visualization.py\",\n",
    "    \"utils/reproducibility.py\",\n",
    "    \"utils/augmentations.py\"\n",
    "]\n",
    "\n",
    "missing = [f for f in files_needed if not os.path.exists(f)]\n",
    "if missing:\n",
    "    print(\"❌ Missing required files:\")\n",
    "    for f in missing:\n",
    "        print(f\"  - {f}\")\n",
    "    print(\"\\nPlease run this notebook from the project root directory\")\n",
    "else:\n",
    "    print(\"✅ All required files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Directory and dataset required\n",
    "\n",
    "If configured properly, the model should have the following directory layout:\n",
    "\n",
    "```\n",
    "deepfake_detector/\n",
    "├── config.py                 # Configuration parameters\n",
    "├── data_loader.py            # Dataset and dataloader implementation\n",
    "├── models/\n",
    "│   ├── __init__.py           # Module initialization\n",
    "│   ├── fingerprint_net.py    # GANFingerprint model architecture\n",
    "│   ├── layers.py             # Custom layers and blocks\n",
    "├── train.py                  # Training script\n",
    "├── evaluate.py               # Evaluation script\n",
    "├── inference.py              # Inference on new images\n",
    "├── utils/\n",
    "│   ├── __init__.py           # Utilities module initialization\n",
    "│   ├── reproducibility.py    # Random seed and reproducibility utilities\n",
    "│   ├── visualization.py      # Plotting and visualization tools\n",
    "│   ├── metrics.py            # Performance metrics calculation\n",
    "│   ├── augmentations.py      # Advanced augmentation techniques\n",
    "|   ├── experiment.py         # Logging of information when training model\n",
    "├── checkpoints/              # Directory for saved model checkpoints\n",
    "├── logs/                     # TensorBoard logs and training records\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset needed and dataset configuration\n",
    "\n",
    "The dataset used to train the model is the 'deepfake and real images' dataset by Manjil Kariki.\n",
    "\n",
    "Link: https://www.kaggle.com/datasets/manjilkarki/deepfake-and-real-images\n",
    "\n",
    "This dataset provides a large dataset of real and deepfake images, split into train, test and validation sets, making it one of the best datasets to be used for deepfake classification models.\n",
    "\n",
    "To configure the dataset, create a file named 'data' in the __root folder__ and follow this directory layout:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "deepfake_detector/\n",
    "├── config.py                 \n",
    "├── data_loader.py\n",
    "data/\n",
    "├── train/\n",
    "│   ├── real/   # Real images\n",
    "│   └── fake/   # Fake/deepfake images\n",
    "├── validation/\n",
    "│   ├── real/\n",
    "│   └── fake/\n",
    "└── test/\n",
    "    ├── real/\n",
    "    └── fake/\n",
    "├── models/\n",
    "│   ├── __init__.py          \n",
    "│   ├── fingerprint_net.py    \n",
    "│   ├── layers.py             \n",
    "├── train.py                  \n",
    "├── evaluate.py               \n",
    "├── inference.py             \n",
    "├── utils/\n",
    "│   ├── __init__.py           \n",
    "│   ├── reproducibility.py    \n",
    "│   ├── visualization.py      \n",
    "│   ├── metrics.py            \n",
    "│   ├── augmentations.py      \n",
    "|   ├── experiment.py         \n",
    "├── checkpoints/              \n",
    "├── logs/                     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Pytorch and CUDA status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch version and CUDA availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "from data_loader import get_dataset_stats\n",
    "from models import FingerprintNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display Current Configuration\n",
    "\n",
    "The model allows for configuration of hyperparameters. The hyperparameters that led to the best results are listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "config.LEARNING_RATE = 5e-5\n",
    "config.WEIGHT_DECAY = 1e-5\n",
    "config.DROPOUT_RATE = 0.40\n",
    "\n",
    "config.DATA_ROOT = \"data\"  # Path to your dataset directory\n",
    "config.BATCH_SIZE = 16     # Adjust based on GPU memory\n",
    "config.NUM_WORKERS = 4     # Number of data loading workers\n",
    "config.NUM_EPOCHS = 20     # Number of training epochs\n",
    "config.EARLY_STOPPING_PATIENCE = 5 # Number of times model will run with no metrics improvement before stopping\n",
    "config.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Display current configuration\n",
    "print(\"Current Configuration:\")\n",
    "print(f\"DATA_ROOT: {config.DATA_ROOT}\")\n",
    "print(f\"INPUT_SIZE: {config.INPUT_SIZE}\")\n",
    "print(f\"BACKBONE: {config.BACKBONE}\")\n",
    "print(f\"BATCH_SIZE: {config.BATCH_SIZE}\")\n",
    "print(f\"EARLY_STOPPING_PATIENCE: {config.EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"LEARNING_RATE: {config.LEARNING_RATE}\")\n",
    "print(f\"WEIGHT_DECAY: {config.WEIGHT_DECAY}\")\n",
    "print(f\"NUM_EPOCHS: {config.NUM_EPOCHS}\")\n",
    "print(f\"DROPOUT_RATE: {config.DROPOUT_RATE}\")\n",
    "print(f\"DEVICE: {config.DEVICE}\")\n",
    "print(f\"USE_AMP: {config.USE_AMP}\")\n",
    "print(f\"CHECKPOINT_DIR: {config.CHECKPOINT_DIR}\")\n",
    "print(f\"LOG_DIR: {config.LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset structure\n",
    "def check_dataset_structure():\n",
    "    paths = [\n",
    "        config.TRAIN_REAL_DIR,\n",
    "        config.TRAIN_FAKE_DIR,\n",
    "        config.VAL_REAL_DIR,\n",
    "        config.VAL_FAKE_DIR,\n",
    "        config.TEST_REAL_DIR,\n",
    "        config.TEST_FAKE_DIR\n",
    "    ]\n",
    "    \n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"❌ {path} does not exist!\")\n",
    "        else:\n",
    "            print(f\"✅ {path} exists with {len(os.listdir(path))} images\")\n",
    "\n",
    "check_dataset_structure()\n",
    "get_dataset_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Display Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some sample images from the dataset\n",
    "def show_samples(real_dir, fake_dir, n=5):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(config.INPUT_SIZE),\n",
    "        transforms.CenterCrop(config.INPUT_SIZE),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Check if directories exist\n",
    "    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n",
    "        print(f\"Error: One or more directories do not exist:\\n{real_dir}\\n{fake_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Get image lists\n",
    "    real_files = os.listdir(real_dir)\n",
    "    fake_files = os.listdir(fake_dir)\n",
    "    \n",
    "    if not real_files or not fake_files:\n",
    "        print(\"Error: One or more directories are empty\")\n",
    "        return\n",
    "    \n",
    "    real_images = [os.path.join(real_dir, f) for f in real_files[:n]]\n",
    "    fake_images = [os.path.join(fake_dir, f) for f in fake_files[:n]]\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for i, img_path in enumerate(real_images + fake_images):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_tensor = transform(img)\n",
    "        \n",
    "        plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(img_tensor.permute(1, 2, 0))\n",
    "        plt.title(\"Real\" if i < n else \"Fake\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show samples from training set\n",
    "try:\n",
    "    show_samples(config.TRAIN_REAL_DIR, config.TRAIN_FAKE_DIR)\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying samples: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize the model with overview of trainable paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = FingerprintNet(backbone=config.BACKBONE)\n",
    "model = model.to(config.DEVICE)\n",
    "\n",
    "# Count model parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print model architecture summary\n",
    "print(model)\n",
    "print(f\"Total trainable parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train as train_function\n",
    "\n",
    "# Wrapper for the training function\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "def train_model(data_root=config.DATA_ROOT, \n",
    "                batch_size=config.BATCH_SIZE, \n",
    "                lr=config.LEARNING_RATE, \n",
    "                epochs=config.NUM_EPOCHS, \n",
    "                backbone=config.BACKBONE,\n",
    "                no_amp=not config.USE_AMP, \n",
    "                resume_checkpoint=None):\n",
    "    \n",
    "    # Override config values if needed\n",
    "    config.DATA_ROOT = data_root\n",
    "    config.BATCH_SIZE = batch_size\n",
    "    config.LEARNING_RATE = lr\n",
    "    config.NUM_EPOCHS = epochs\n",
    "    config.BACKBONE = backbone\n",
    "    config.USE_AMP = not no_amp\n",
    "    \n",
    "    # Create args object\n",
    "    args = Args(\n",
    "        data_root=data_root,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        epochs=epochs,\n",
    "        backbone=backbone,\n",
    "        no_amp=no_amp,\n",
    "        resume_checkpoint=resume_checkpoint\n",
    "    )\n",
    "    \n",
    "    # Call the training function\n",
    "    train_function(args)\n",
    "    \n",
    "    # Return the path to the best checkpoint\n",
    "    return os.path.join(config.CHECKPOINT_DIR, f\"ganfingerprint_best.pth\")\n",
    "\n",
    "# Train the model \n",
    "best_checkpoint = train_model()\n",
    "\n",
    "# To resume training from a checkpoint (uncomment to run):\n",
    "# best_checkpoint = train_model(resume_checkpoint=\"checkpoints/ganfingerprint_epoch10.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluating the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate as evaluate_function\n",
    "\n",
    "# Wrapper for the evaluation function\n",
    "def evaluate_model(checkpoint_path, output_dir=\"eval_results\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the model checkpoint\n",
    "        output_dir: Directory to save evaluation results\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Call the evaluation function\n",
    "    evaluate_function(checkpoint_path, output_dir)\n",
    "        \n",
    "    # Display the generated images\n",
    "    image_paths = [\n",
    "        os.path.join(output_dir, \"confusion_matrix.png\"),\n",
    "        os.path.join(output_dir, \"roc_curve.png\"),\n",
    "        os.path.join(output_dir, \"precision_recall_curve.png\")\n",
    "    ]\n",
    "    \n",
    "\n",
    "# Evaluate the model (Insert relative directory to trained model in checkpoints folder to run the evaluation)\n",
    "evaluate_model(\"checkpoints\\Run_19_learning_rate_0.00005\\ganfingerprint_20250411_125832_best.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Testing the model with actual images\n",
    "\n",
    "After training and evaluating the model, we can try out the capabilities of the model by having it classify any image that is outside of the dataset. Lets see if it is able to classify images correctly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single image inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.serialization\n",
    "import os\n",
    "from inference import run_inference\n",
    "\n",
    "# Add numpy scalar to safe globals for PyTorch 2.6+ compatibility\n",
    "torch.serialization.add_safe_globals(['numpy._core.multiarray.scalar'])\n",
    "\n",
    "# Function to run single image inference\n",
    "def run_single_inference(checkpoint_path, image_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    Run inference on a single image using functions from inference.py\n",
    "    \"\"\"\n",
    "    # Fix path separators if needed\n",
    "    checkpoint_path = checkpoint_path.replace('\\\\', '/')\n",
    "    image_path = image_path.replace('\\\\', '/')\n",
    "    if output_dir:\n",
    "        output_dir = output_dir.replace('\\\\', '/')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Run inference\n",
    "    print(f\"Running inference on: {image_path}\")\n",
    "    print(f\"Using checkpoint: {checkpoint_path}\")\n",
    "    run_inference(checkpoint_path, image_path, output_dir, batch_mode=False)\n",
    "\n",
    "model_checkpoint = \"checkpoints\\Run_19_learning_rate_0.00005\\Ganfingerprint_20250411_125832_best.pth\"\n",
    "test_image_path =  \"data/test/fake/fake_23.jpg\"\n",
    "\n",
    "run_inference(model_checkpoint, test_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_inference(checkpoint_path, image_dir, output_dir=\"inference_results\"):\n",
    "    \"\"\"\n",
    "    Run inference on a directory of images using functions from inference.py\n",
    "    \"\"\"\n",
    "    # Fix path separators if needed\n",
    "    checkpoint_path = checkpoint_path.replace('\\\\', '/')\n",
    "    image_dir = image_dir.replace('\\\\', '/')\n",
    "    output_dir = output_dir.replace('\\\\', '/')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Run inference\n",
    "    print(f\"Running batch inference on directory: {image_dir}\")\n",
    "    print(f\"Using checkpoint: {checkpoint_path}\")\n",
    "    run_inference(checkpoint_path, image_dir, output_dir, batch_mode=True)\n",
    "\n",
    "# Run inference on a single image (uncomment to run)\n",
    "test_image_path = \"test_images/fake_21.jpg\"\n",
    "checkpoint_path = \"checkpoints/Run_19_learning_rate_0.00005/ganfingerprint_20250411_125832_best.pth\"\n",
    "run_single_inference(checkpoint_path, test_image_path, \"inference_results\")\n",
    "\n",
    "# Run inference on a directory of images (uncomment to run)\n",
    "# test_dir = \"test_images\"\n",
    "# run_batch_inference(checkpoint_path, test_dir, \"inference_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Interactive image upload and prediction (using ipywidgets)\n",
    "# from ipywidgets import FileUpload, Button, Output, HBox, VBox, HTML\n",
    "# from IPython.display import display, clear_output\n",
    "# import io\n",
    "\n",
    "# def create_interactive_demo(checkpoint_path):\n",
    "#     # Load model\n",
    "#     model = FingerprintNet(backbone=config.BACKBONE)\n",
    "#     checkpoint = torch.load(checkpoint_path, map_location=config.DEVICE, weights_only=False)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     model = model.to(config.DEVICE)\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Transform for inference\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize(config.INPUT_SIZE),\n",
    "#         transforms.CenterCrop(config.INPUT_SIZE),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "    \n",
    "#     # Create widgets\n",
    "#     file_upload = FileUpload(accept='image/*', multiple=False)\n",
    "#     predict_button = Button(description='Predict')\n",
    "#     output = Output()\n",
    "    \n",
    "#     # Define button click handler\n",
    "#     def on_predict_button_clicked(b):\n",
    "#         with output:\n",
    "#             clear_output()\n",
    "            \n",
    "#             if not file_upload.value:\n",
    "#                 print(\"Please upload an image first.\")\n",
    "#                 return\n",
    "            \n",
    "#             # Get the uploaded file\n",
    "#             uploaded_file = list(file_upload.value.values())[0]\n",
    "#             content = uploaded_file['content']\n",
    "            \n",
    "#             # Convert to PIL Image\n",
    "#             image = Image.open(io.BytesIO(content)).convert('RGB')\n",
    "            \n",
    "#             # Preprocess and make prediction\n",
    "#             image_tensor = transform(image).unsqueeze(0).to(config.DEVICE)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 if config.USE_AMP:\n",
    "#                     with torch.cuda.amp.autocast():\n",
    "#                         output = model(image_tensor)\n",
    "#                 else:\n",
    "#                     output = model(image_tensor)\n",
    "            \n",
    "#             # Get probability and class\n",
    "#             prob = torch.sigmoid(output).item()\n",
    "#             pred_class = \"Real\" if prob >= 0.5 else \"Fake\"\n",
    "            \n",
    "#             # Display results\n",
    "#             color = 'green' if pred_class == 'Real' else 'red'\n",
    "#             display(HTML(f\"<h3 style='color:{color}'>Prediction: {pred_class} (Confidence: {prob:.4f})</h3>\"))\n",
    "            \n",
    "#             # Display image\n",
    "#             plt.figure(figsize=(8, 8))\n",
    "#             plt.imshow(image)\n",
    "#             plt.axis('off')\n",
    "#             plt.title(f\"Prediction: {pred_class} ({prob:.4f})\", color=color, fontsize=16)\n",
    "#             plt.show()\n",
    "    \n",
    "#     # Connect the button click event\n",
    "#     predict_button.on_click(on_predict_button_clicked)\n",
    "    \n",
    "#     # Create UI layout\n",
    "#     display(HTML(\"<h2>Deepfake Detection Demo</h2>\"))\n",
    "#     display(HTML(\"<p>Upload an image and click 'Predict' to detect if it's real or a GAN-generated fake.</p>\"))\n",
    "#     display(HBox([file_upload, predict_button]))\n",
    "#     display(output)\n",
    "\n",
    "# # Run the interactive demo (uncomment to run)\n",
    "# create_interactive_demo(\"checkpoints\\Run_19_learning_rate_0.00005\\ganfingerprint_20250411_125832_best.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
